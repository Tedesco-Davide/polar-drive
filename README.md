# PolarDrive üöó‚ùÑÔ∏è

Repository unificato per il progetto **PolarDrive**.

---

## ‚öôÔ∏è Comandi

### üî∑ FRONTEND GENERICO

_(Tasto destro sulla cartella `frontend` ‚Üí Open in integrated Terminal)_

- `npm run dev` ‚Üí Avvio classico in dev
- `npm lista` ‚Üí Visualizza tutti i pacchetti installati
- `npm i` ‚Üí Reinstalla tutti i pacchetti

---

### üî∂ BACKEND GENERICO

_(Tasto destro sulla cartella `backend` ‚Üí Open in integrated Terminal)_

- `dotnet build` ‚Üí Rebuild completo della soluzione
- `dotnet run --project PolarDriveInitDB.Cli` ‚Üí Crea un nuovo DB da zero nella cartella `PolarDriveInitDB.Cli`
- `dotnet run --project PolarDriveInitDBMockData.Cli` ‚Üí Aggiunge dati di mock al DB creato
- `dotnet run --project PolarDrive.WebApi` ‚Üí Avvia la WebAPI principale

---

### üöó TESLA MOCK API SERVICE

_(Tasto destro sulla cartella `backend/PolarDrive.TeslaMockApiService` ‚Üí Open in integrated Terminal)_

- `dotnet run` ‚Üí Avvia il servizio di push dati verso la WebAPI
  > Simula i dati Tesla e li invia automaticamente alla WebAPI principale per test e sviluppo.

---

### üß† BACKEND MISTRAL AI

_(Tasto destro sulla cartella `backend/PolarDrive.WebApi` ‚Üí Open in integrated Terminal)_

#### ‚úÖ Ollama

- `ollama --version` ‚Üí Mostra la versione del runtime **Ollama**
  > Ollama √® il **motore AI locale**: scarica, avvia e gestisce modelli LLM direttamente sul tuo PC.

#### üß™ DEBUG LOCALE (opzionale)

- `ollama run mistral` ‚Üí Esegue **Mistral** in modalit√† chat interattiva (terminal-based)
  > ‚ö†Ô∏è Solo per test manuali: non adatto all'uso via codice .NET.

#### ‚úÖ UTILIZZO CORRETTO IN BACKEND

- `ollama serve` ‚Üí Avvia **Ollama in modalit√† server REST**
  > Espone l'endpoint `http://localhost:11434/api/generate` per richieste AI programmatiche.  
  > ‚úîÔ∏è √à **stateless** e perfettamente integrabile nel backend (`HttpClient`, `POST`, JSON).

##### üîç Come verificare se √® attivo

- Apri il browser e visita:  
  `http://localhost:11434`

- Oppure in terminale Powershell integrato su Visual Studio:

  ```bash
  Invoke-RestMethod -Uri http://localhost:11434/api/generate -Method Post -Body '{"model":"mistral","prompt":"Sei attivo?","stream":false}' -ContentType "application/json"
  ```

- Oppure via `curl`:

  ```bash
  curl http://localhost:11434/api/generate -d "{\"model\": \"mistral\", \"prompt\": \"Sei attivo?\", \"stream\": false}"
  ```

- Puoi chiudere Ollama manualmente nel terminale Powershell integrato su Visual Studio:

  ```bash
  Stop-Process -Name ollama -Force
  ```
