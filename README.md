# PolarDrive ğŸš—â„ï¸

Repository unificato per il progetto **PolarDrive**.

---

## âš™ï¸ Comandi

### ğŸ”· FRONTEND GENERICO

_(Tasto destro sulla cartella `frontend` â†’ Open in integrated Terminal)_

- `npm run dev` â†’ Avvio classico in dev
- `npm lista` â†’ Visualizza tutti i pacchetti installati
- `npm i` â†’ Reinstalla tutti i pacchetti

---

### ğŸ”¶ BACKEND GENERICO

_(Tasto destro sulla cartella `backend` â†’ Open in integrated Terminal)_

- `dotnet build` â†’ Rebuild completo della soluzione
- `dotnet run --project PolarDriveInitDB.Cli` â†’ Crea un nuovo DB da zero nella cartella `PolarDriveInitDB.Cli`
- `dotnet run --project PolarDriveInitDBMockData.Cli` â†’ Aggiunge dati di mock al DB creato
- `dotnet run --project PolarDrive.WebApi` â†’ Avvia la WebAPI principale

---

### ğŸš— TESLA MOCK API SERVICE

_(Tasto destro sulla cartella `backend/PolarDrive.TeslaMockApiService` â†’ Open in integrated Terminal)_

- `dotnet run` â†’ Avvia il servizio di push dati verso la WebAPI
  > Simula i dati Tesla e li invia automaticamente alla WebAPI principale per test e sviluppo.

---

### ğŸ§  BACKEND PolarAi

_(Tasto destro sulla cartella `backend/PolarDrive.WebApi` â†’ Open in integrated Terminal)_

#### âœ… Ollama

- `ollama --version` â†’ Mostra la versione del runtime **Ollama**
  > Ollama Ã¨ il **motore AI locale**: scarica, avvia e gestisce modelli LLM direttamente sul tuo PC.

#### ğŸ§ª UTILIZZO CORRETTO IN BACKEND

- `ollama serve` â†’ Avvia **Ollama in modalitÃ  server REST**
  > Espone l'endpoint `http://localhost:11434/api/generate` per richieste AI programmatiche.  
  > âœ”ï¸ Ãˆ **stateless** e perfettamente integrabile nel backend (`HttpClient`, `POST`, JSON).

##### ğŸ” Come verificare se Ã¨ attivo

- Apri il browser e visita:  
  `http://localhost:11434`
