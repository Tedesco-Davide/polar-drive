# PolarDrive ğŸš—â„ï¸

Repository per il progetto **PolarDrive**.

---

### ğŸ³ DOCKER

_(Tasto destro sulla cartella ROOT principale del progetto â†’ Open in integrated Terminal)_

Crea la rete con nome esplicito (comando da lanciare una sola volta)

- docker network create polardrive-network-dev
- docker network create polardrive-network-prod

DEV => REBUILD FRONTEND POST MODIFICHE =>

- STOP CONTAINER DEV => docker compose -f docker-compose.dev.yml down
- REBUILD IMMAGINE DEV FRONTEND => docker build -f frontend/Dockerfile -t polardrive-frontend:latest .
- RESTART CONTAINER DEV => docker compose -f docker-compose.dev.yml --env-file .env.dev up -d
- LOGS => docker compose -f docker-compose.dev.yml logs -f frontend

DEV => REBUILD TESLA-MOCK-API-SERVICE POST MODIFICHE =>

- STOP CONTAINER DEV â‡’ docker compose -f docker-compose.dev.yml down
- REBUILD IMMAGINE TESLA MOCK â‡’ docker build -f backend/PolarDrive.TeslaMockApiService/Dockerfile -t polardrive-mock:latest .
- RESTART CONTAINER DEV â‡’ docker compose -f docker-compose.dev.yml --env-file .env.dev up -d
- LOGS â‡’ docker compose -f docker-compose.dev.yml logs -f mock

DEV => REBUILD POLARDRIVE-WEB-API POST MODIFICHE =>

- STOP CONTAINER DEV â‡’ docker compose -f docker-compose.dev.yml down
- REBUILD IMMAGINE WEB-API â‡’ docker build -f backend/PolarDrive.WebApi/Dockerfile -t polardrive-api:latest .
- RESTART CONTAINER DEV â‡’ docker compose -f docker-compose.dev.yml --env-file .env.dev up -d
- LOGS â‡’ docker compose -f docker-compose.dev.yml logs -f api

### ğŸ”· FRONTEND POLARDRIVE ADMIN

_(Tasto destro sulla cartella `frontend` â†’ Open in integrated Terminal)_

#### **ğŸš€ SVILUPPO (Development)**

- `npm i` â†’ Installa/reinstalla tutti i pacchetti
- `npm run dev` â†’ Avvio in modalitÃ  sviluppo (hot reload, debug attivo)
- `npm list` â†’ Visualizza tutti i pacchetti installati

### ğŸ”¶ BACKEND PolarAi

_(Tasto destro sulla cartella `backend` â†’ Open in integrated Terminal)_

#### **ğŸš€ SVILUPPO (Development)**

- `dotnet build` â†’ Rebuild completo della soluzione

    - `$env:ASPNETCORE_ENVIRONMENT="Development"` â†’ IMPORTANTISSIMO â†’ Set di ambiente DEV

    - `dotnet run --project PolarDriveInitDB.Cli` â†’ Crea un nuovo DB (mock, cancellabile, per tesing) da ZERO in Micrisoft SQL Server

    - `dotnet run --project PolarDriveInitDBMockData.Cli` â†’ Aggiunge dati di mock al DB creato (opzionale)

    - `dotnet run --project PolarDrive.WebApi` â†’ Avvia la WebAPI principale in modalitÃ  sviluppo

    > Espone l'endpoint `http://localhost:3000/admin` per Dashboard Backend

- Extra comandi da lancaire nel caso di problemi di connessione al backend

    > dotnet dev-certs https --clean

    > dotnet dev-certs https --trust
    
    > dotnet dev-certs https --check

---

#### **ğŸ“¦ PRODUZIONE (Production)**

- `$env:ASPNETCORE_ENVIRONMENT="Production"` â†’ IMPORTANTISSIMO â†’ Set di ambiente PROD
- `dotnet run --project PolarDriveInitDB.Cli` â†’ Crea un nuovo DB da zero ( di PRODUZIONE ) da ZERO in Micrisoft SQL Server
- â†’ IMPORTANTISSIMO â†’ MAI eseguire questo comando se NON per RESETTARE IL DB DI PRODUZIONE

---

### ğŸ§ª TEST AUTOMATICO API

_(Tasto destro sulla cartella `frontend` â†’ Open in integrated Terminal)_

- `node converter.js` â†’ Lanciare questo comando ogni volta che si aggiorna il file converter.js â†’ Genera collection Postman ottimizzata con parametri e body corretti
- `newman run polardrive-collection-fixed.json --insecure` â†’ Testa tutti gli endpoint automaticamente

---

### ğŸš— TESLA MOCK API SERVICE

_(Tasto destro sulla cartella `backend/PolarDrive.TeslaMockApiService` â†’ Open in integrated Terminal)_

- `dotnet run` â†’ Avvia il servizio di push dati verso la WebAPI
  > Simula i dati Tesla e li invia automaticamente alla WebAPI principale per test e sviluppo.

---

#### âœ… Ollama

_(Tasto destro sulla cartella `backend` â†’ Open in integrated Terminal)_

- `ollama --version` â†’ Mostra la versione del runtime **Ollama**

  > Ollama Ã¨ il **motore AI locale**: scarica, avvia e gestisce modelli LLM direttamente sul tuo PC.

- `ollama serve` â†’ Avvia **Ollama in modalitÃ  server REST**

  > Espone l'endpoint `http://localhost:11434/api/generate` per richieste AI programmatiche.  
  > âœ”ï¸ Ãˆ **stateless** e perfettamente integrabile nel backend (`HttpClient`, `POST`, JSON).

##### ğŸ” Come verificare se Ã¨ attivo

- Apri il browser e visita:  
  `http://localhost:11434`
